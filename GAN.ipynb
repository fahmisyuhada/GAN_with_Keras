{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtm1mLH3EnhSbnnouMJ0L1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahmisyuhada/GAN_with_Keras/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Adversarial Networks (GAN)\n",
        "\n",
        "Ref: https://github.com/bnsreenu/python_for_microscopists/blob/master/125_126_GAN_training_mnist.py"
      ],
      "metadata": {
        "id": "jlbZDX1L3Y51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM8_wj4rEPkY",
        "outputId": "d9e4b4db-b615-45d6-ce07-884404bb914c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load number images of mnist dataset from keras <br>\n",
        "`x,y = mnist.load_data()`<br>\n",
        "> where:<br>\n",
        " * x = Training dataset <br>\n",
        " * y = Testing dataset <br>\n",
        " * x[0] = Image data of each training dataset<br>\n",
        " * x[1] = Label data of each training dataset<br>\n",
        " * y[0] = Image data of each testing dataset <br>\n",
        " * y[1] = label data of each testing dataset<br>"
      ],
      "metadata": {
        "id": "ETrYTfCeMoz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = mnist.load_data()  # Return data x = data train y = data testing  index 0 for data and 1 for label\n",
        "plt.imshow(x[0][0],\"gray\") # Show data train dataset index 0\n",
        "print(x[1][0]) # Print label train dataset index 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "uv8zAb2EFQqY",
        "outputId": "d097f7f4-7277-4993-f08c-7738fd852beb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define input image dimensions\n",
        "# image dimensions must be the same with dataset\n",
        "# large inmages will take too much time and rosource\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "img_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6jsFtZeLHNt",
        "outputId": "8d9b12f0-68f0-41aa-8c90-1f356e383e18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given input of noise (Latent) vector, the Generator produces an image\n",
        "def build_generator():\n",
        "  noise_shape = (100,) # 1D array of size 100 (latent vector / noise)\n",
        "\n",
        "# Define your generator network\n",
        "# Here we are only using Dense layers. But network can be complicated based\n",
        "# on the application. For Example, you can useVGG for super res. Gan\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Dense(256, input_shape=noise_shape))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "  model.add(Dense(1024))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "  model.add(Reshape(img_shape))\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  noise = Input(shape=noise_shape)\n",
        "  img = model(noise)\n",
        "\n",
        "  return Model(noise, img)\n",
        "\n",
        "#Alpha — α is a hyperparameter which controls the underlying value to which the\n",
        "#function saturates negatives network inputs.\n",
        "#Momentum — Speed up the training\n",
        "##########################################################################\n",
        "\n",
        "#Given an input image, the Discriminator outputs the likelihood of the image being real.\n",
        "    #Binary classification - true or false (we're calling it validity)\n",
        "\n",
        "def build_discriminator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.summary()\n",
        "\n",
        "  img = Input(shape=img_shape)\n",
        "  validity = model(img)\n",
        "\n",
        "  return Model(img, validity)\n",
        "\n",
        "#The validity is the Discriminator’s guess of input being real or not.\n",
        "\n",
        "# Now that we have constructed our two models it’s time to pit them against each other.\n",
        "# We do this by defining a training function, loading the data set, re-scaling our training\n",
        "# images and setting the ground truths.\n",
        "def train(epochs, batch_size=128, save_interval=50):\n",
        "  (X_train, _), (_, _) = mnist.load_data() # Load the dataset\n",
        "\n",
        "  # Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n",
        "\n",
        "  X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "  # Add channels dimension. As the input to our gen and discr. has a shape 28x28x1.\n",
        "  X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "  half_batch = int(batch_size / 2)\n",
        "\n",
        "  # We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "  # a random batch of images from our true dataset, generating a set of images from our\n",
        "  # Generator, feeding both set of images into our Discriminator, and finally setting the\n",
        "  # loss parameters for both the real and fake images, as well as the combined loss.\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # ---------------------\n",
        "    #  Train Discriminator\n",
        "    # ---------------------\n",
        "\n",
        "    # Select a random half batch of real images\n",
        "\n",
        "    idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "    imgs = X_train[idx]\n",
        "\n",
        "    noise = np.random.normal(0, 1, (half_batch, 100))\n",
        "\n",
        "    # Generate a half batch of fake images\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Train the discriminator on real and fake images, separately\n",
        "    # Research showed that separate training is more effective.\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
        "\n",
        "    # take average loss from real and fake images.\n",
        "    #\n",
        "\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # And within the same loop we train our Generator, by setting the input noise and\n",
        "    # ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "    # by specifying the gradient loss.\n",
        "\n",
        "    # ---------------------\n",
        "    #  Train Generator\n",
        "    # ---------------------\n",
        "\n",
        "    # Create noise vectors as input for generator.\n",
        "    # Create as many noise vectors as defined by the batch size.\n",
        "    # Based on normal distribution. Output will be of size (batch size, 100)\n",
        "\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "    # The generator wants the discriminator to label the generated samples\n",
        "    # as valid (ones)\n",
        "    # This is where the genrator is trying to trick discriminator into believing\n",
        "    # the generated image is true (hence value of 1 for y)\n",
        "\n",
        "    valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "    # Generator is part of combined where it got directly linked with the discriminator\n",
        "    # Train the generator with noise as x and 1 as y.\n",
        "    # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "    # job of folling the discriminator then the output would be 1 (true)\n",
        "\n",
        "    g_loss = combined.train_on_batch(noise, valid_y)\n",
        "\n",
        "    # Additionally, in order for us to keep track of our training process, we print the\n",
        "    # progress and save the sample image output depending on the epoch interval specified.\n",
        "    # Plot the progress\n",
        "\n",
        "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "    # If at save interval => save generated image samples\n",
        "    if epoch % save_interval == 0:\n",
        "      save_imgs(epoch)\n",
        "\n",
        "    # when the specific sample_interval is hit, we call the\n",
        "    # sample_image function. Which looks as follows.\n",
        "\n",
        "def save_imgs(epoch):\n",
        "  r, c = 5, 5\n",
        "  noise = np.random.normal(0, 1, (r * c, 100))\n",
        "  gen_imgs = generator.predict(noise)\n",
        "\n",
        "  # Rescale images 0 - 1\n",
        "  gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  cnt = 0\n",
        "  for i in range(r):\n",
        "    for j in range(c):\n",
        "      axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "      axs[i,j].axis('off')\n",
        "      cnt += 1\n",
        "  fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
        "  plt.close()\n",
        "\n",
        "  #This function saves our images for us to view\n",
        "\n",
        "\n",
        "print(\"Done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XRGGgYK8AE1",
        "outputId": "fcdfa673-5553-48d8-f09c-93f79d5349c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "\n",
        "#Let us also define our optimizer for easy use later on.\n",
        "#That way if you change your mind, you can change it easily here\n",
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n",
        "\n",
        "# Build and compile the discriminator first.\n",
        "#Generator will be trained as part of the combined model, later.\n",
        "#pick the loss function and the type of metric to keep track.\n",
        "#Binary cross entropy as we are doing prediction and it is a better\n",
        "#loss function compared to MSE or other.\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "#build and compile our Discriminator, pick the loss function\n",
        "\n",
        "#SInce we are only generating (faking) images, let us not track any metrics.\n",
        "generator = build_generator()\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "##This builds the Generator and defines the input noise.\n",
        "#In a GAN the Generator network takes noise z as an input to produce its images.\n",
        "z = Input(shape=(100,))   #Our random input to the generator\n",
        "img = generator(z)\n",
        "\n",
        "#This ensures that when we combine our networks we only train the Generator.\n",
        "#While generator training we do not want discriminator weights to be adjusted.\n",
        "#This Doesn't affect the above descriminator training.\n",
        "discriminator.trainable = False\n",
        "\n",
        "#This specifies that our Discriminator will take the images generated by our Generator\n",
        "#and true dataset and set its output to a parameter called valid, which will indicate\n",
        "#whether the input is real or not.\n",
        "valid = discriminator(img)  #Validity check on the generated image\n",
        "\n",
        "\n",
        "#Here we combined the models and also set our loss function and optimizer.\n",
        "#Again, we are only training the generator here.\n",
        "#The ultimate goal here is for the Generator to fool the Discriminator.\n",
        "# The combined model  (stacked generator and discriminator) takes\n",
        "# noise as input => generates images => determines validity\n",
        "\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "train(epochs=1000, batch_size=32, save_interval=10)\n",
        "\n",
        "#Save model for future use to generate fake images\n",
        "#Not tested yet... make sure right model is being saved..\n",
        "#Compare with GAN4\n",
        "\n",
        "generator.save('generator_model.h5')  #Test the model on GAN4_predict...\n",
        "#Change epochs back to 30K\n",
        "\n",
        "#Epochs dictate the number of backward and forward propagations, the batch_size\n",
        "#indicates the number of training samples per backward/forward propagation, and the\n",
        "#sample_interval specifies after how many epochs we call our sample_image function."
      ],
      "metadata": {
        "id": "gaZxe2dO5t09",
        "outputId": "683505dc-1441-444a-e6a3-1845915a934b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_4 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 512)               401920    \n",
            "                                                                 \n",
            " leaky_re_lu_20 (LeakyReLU)  (None, 512)               0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " leaky_re_lu_21 (LeakyReLU)  (None, 256)               0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_31 (Dense)            (None, 256)               25856     \n",
            "                                                                 \n",
            " leaky_re_lu_22 (LeakyReLU)  (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_23 (LeakyReLU)  (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 1024)              525312    \n",
            "                                                                 \n",
            " leaky_re_lu_24 (LeakyReLU)  (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 784)               803600    \n",
            "                                                                 \n",
            " reshape_4 (Reshape)         (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "0 [D loss: 0.597541, acc.: 50.00%] [G loss: 0.400051]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1 [D loss: 0.415598, acc.: 59.38%] [G loss: 0.436427]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "2 [D loss: 0.384840, acc.: 59.38%] [G loss: 0.484879]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "3 [D loss: 0.395868, acc.: 56.25%] [G loss: 0.582243]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "4 [D loss: 0.398479, acc.: 59.38%] [G loss: 0.688814]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "5 [D loss: 0.337314, acc.: 81.25%] [G loss: 0.741336]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "6 [D loss: 0.324206, acc.: 87.50%] [G loss: 0.992540]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "7 [D loss: 0.259208, acc.: 96.88%] [G loss: 1.109408]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "8 [D loss: 0.214797, acc.: 100.00%] [G loss: 1.280929]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "9 [D loss: 0.180453, acc.: 100.00%] [G loss: 1.551452]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "10 [D loss: 0.140181, acc.: 100.00%] [G loss: 1.683823]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "11 [D loss: 0.108783, acc.: 100.00%] [G loss: 1.838557]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "12 [D loss: 0.108078, acc.: 100.00%] [G loss: 1.881055]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "13 [D loss: 0.102592, acc.: 100.00%] [G loss: 2.123857]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "14 [D loss: 0.072028, acc.: 100.00%] [G loss: 2.233828]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "15 [D loss: 0.079302, acc.: 100.00%] [G loss: 2.323419]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "16 [D loss: 0.064537, acc.: 100.00%] [G loss: 2.475264]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "17 [D loss: 0.065266, acc.: 100.00%] [G loss: 2.530656]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "18 [D loss: 0.056947, acc.: 100.00%] [G loss: 2.669086]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "19 [D loss: 0.045185, acc.: 100.00%] [G loss: 2.774700]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "20 [D loss: 0.038655, acc.: 100.00%] [G loss: 2.844629]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "21 [D loss: 0.039715, acc.: 100.00%] [G loss: 2.882971]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "22 [D loss: 0.038710, acc.: 100.00%] [G loss: 2.848351]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "23 [D loss: 0.037893, acc.: 100.00%] [G loss: 2.894419]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "24 [D loss: 0.038178, acc.: 100.00%] [G loss: 3.031977]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "25 [D loss: 0.028475, acc.: 100.00%] [G loss: 3.169267]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "26 [D loss: 0.034276, acc.: 100.00%] [G loss: 3.178567]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "27 [D loss: 0.034678, acc.: 100.00%] [G loss: 3.257244]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "28 [D loss: 0.020788, acc.: 100.00%] [G loss: 3.242112]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "29 [D loss: 0.028654, acc.: 100.00%] [G loss: 3.389264]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "30 [D loss: 0.028481, acc.: 100.00%] [G loss: 3.215739]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "31 [D loss: 0.027723, acc.: 100.00%] [G loss: 3.454566]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "32 [D loss: 0.022778, acc.: 100.00%] [G loss: 3.483479]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "33 [D loss: 0.019007, acc.: 100.00%] [G loss: 3.452251]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "34 [D loss: 0.021784, acc.: 100.00%] [G loss: 3.323558]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "35 [D loss: 0.021083, acc.: 100.00%] [G loss: 3.434282]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "36 [D loss: 0.024814, acc.: 100.00%] [G loss: 3.643441]\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "37 [D loss: 0.015820, acc.: 100.00%] [G loss: 3.745671]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "38 [D loss: 0.021770, acc.: 100.00%] [G loss: 3.625968]\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "39 [D loss: 0.012712, acc.: 100.00%] [G loss: 3.632879]\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "40 [D loss: 0.021092, acc.: 100.00%] [G loss: 3.603453]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "41 [D loss: 0.014429, acc.: 100.00%] [G loss: 3.811960]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "42 [D loss: 0.020236, acc.: 100.00%] [G loss: 3.783442]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "43 [D loss: 0.015595, acc.: 100.00%] [G loss: 3.745992]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "44 [D loss: 0.016467, acc.: 100.00%] [G loss: 3.874887]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "45 [D loss: 0.012605, acc.: 100.00%] [G loss: 3.708853]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "46 [D loss: 0.021054, acc.: 100.00%] [G loss: 3.785367]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "47 [D loss: 0.012977, acc.: 100.00%] [G loss: 3.894520]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "48 [D loss: 0.019737, acc.: 100.00%] [G loss: 3.976778]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "49 [D loss: 0.012884, acc.: 100.00%] [G loss: 4.032996]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "50 [D loss: 0.014677, acc.: 100.00%] [G loss: 3.959459]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "51 [D loss: 0.015390, acc.: 100.00%] [G loss: 3.911082]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "52 [D loss: 0.014385, acc.: 100.00%] [G loss: 4.133088]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "53 [D loss: 0.013193, acc.: 100.00%] [G loss: 4.101050]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "54 [D loss: 0.014933, acc.: 100.00%] [G loss: 4.010988]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "55 [D loss: 0.011475, acc.: 100.00%] [G loss: 3.988638]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "56 [D loss: 0.013785, acc.: 100.00%] [G loss: 3.997747]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "57 [D loss: 0.010174, acc.: 100.00%] [G loss: 4.027461]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "58 [D loss: 0.017447, acc.: 100.00%] [G loss: 4.115392]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "59 [D loss: 0.015794, acc.: 100.00%] [G loss: 3.993838]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "60 [D loss: 0.017095, acc.: 100.00%] [G loss: 4.162823]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "61 [D loss: 0.012430, acc.: 100.00%] [G loss: 4.081684]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "62 [D loss: 0.015742, acc.: 100.00%] [G loss: 4.178006]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "63 [D loss: 0.015300, acc.: 100.00%] [G loss: 4.048137]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "64 [D loss: 0.018688, acc.: 100.00%] [G loss: 4.161783]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "65 [D loss: 0.011913, acc.: 100.00%] [G loss: 4.278702]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "66 [D loss: 0.010993, acc.: 100.00%] [G loss: 4.204495]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "67 [D loss: 0.013808, acc.: 100.00%] [G loss: 4.173335]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "68 [D loss: 0.007523, acc.: 100.00%] [G loss: 4.161345]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "69 [D loss: 0.016218, acc.: 100.00%] [G loss: 4.217061]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "70 [D loss: 0.012546, acc.: 100.00%] [G loss: 4.231283]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "71 [D loss: 0.011493, acc.: 100.00%] [G loss: 4.221977]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "72 [D loss: 0.021094, acc.: 100.00%] [G loss: 4.163071]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "73 [D loss: 0.010687, acc.: 100.00%] [G loss: 4.329132]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "74 [D loss: 0.014588, acc.: 100.00%] [G loss: 4.439802]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "75 [D loss: 0.013591, acc.: 100.00%] [G loss: 4.421133]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "76 [D loss: 0.009537, acc.: 100.00%] [G loss: 4.372632]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "77 [D loss: 0.014620, acc.: 100.00%] [G loss: 4.292348]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "78 [D loss: 0.005672, acc.: 100.00%] [G loss: 4.449433]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "79 [D loss: 0.010646, acc.: 100.00%] [G loss: 4.414584]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "80 [D loss: 0.009562, acc.: 100.00%] [G loss: 4.286676]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "81 [D loss: 0.013005, acc.: 100.00%] [G loss: 4.359312]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "82 [D loss: 0.012310, acc.: 100.00%] [G loss: 4.297149]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "83 [D loss: 0.010567, acc.: 100.00%] [G loss: 4.278388]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "84 [D loss: 0.022491, acc.: 100.00%] [G loss: 4.291374]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "85 [D loss: 0.012679, acc.: 100.00%] [G loss: 4.369585]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "86 [D loss: 0.015390, acc.: 100.00%] [G loss: 4.585063]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "87 [D loss: 0.007020, acc.: 100.00%] [G loss: 4.601069]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "88 [D loss: 0.010183, acc.: 100.00%] [G loss: 4.436317]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "89 [D loss: 0.015950, acc.: 100.00%] [G loss: 4.469839]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "90 [D loss: 0.026654, acc.: 100.00%] [G loss: 4.580769]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "91 [D loss: 0.013355, acc.: 100.00%] [G loss: 4.793730]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "92 [D loss: 0.012629, acc.: 100.00%] [G loss: 4.588603]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "93 [D loss: 0.011497, acc.: 100.00%] [G loss: 4.669776]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "94 [D loss: 0.010096, acc.: 100.00%] [G loss: 4.628176]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "95 [D loss: 0.007496, acc.: 100.00%] [G loss: 4.828113]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "96 [D loss: 0.008885, acc.: 100.00%] [G loss: 4.618636]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "97 [D loss: 0.015682, acc.: 100.00%] [G loss: 4.828636]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "98 [D loss: 0.008229, acc.: 100.00%] [G loss: 4.658529]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "99 [D loss: 0.009510, acc.: 100.00%] [G loss: 4.542569]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "100 [D loss: 0.016965, acc.: 100.00%] [G loss: 4.687324]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "101 [D loss: 0.005239, acc.: 100.00%] [G loss: 4.513962]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "102 [D loss: 0.028163, acc.: 100.00%] [G loss: 4.764691]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "103 [D loss: 0.037027, acc.: 100.00%] [G loss: 4.673368]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "104 [D loss: 0.015397, acc.: 100.00%] [G loss: 4.898735]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "105 [D loss: 0.015035, acc.: 100.00%] [G loss: 5.158345]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "106 [D loss: 0.021786, acc.: 100.00%] [G loss: 4.628575]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "107 [D loss: 0.008322, acc.: 100.00%] [G loss: 4.814981]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "108 [D loss: 0.013519, acc.: 100.00%] [G loss: 4.925978]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "109 [D loss: 0.010408, acc.: 100.00%] [G loss: 5.063457]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "110 [D loss: 0.026882, acc.: 100.00%] [G loss: 4.490602]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "111 [D loss: 0.009500, acc.: 100.00%] [G loss: 4.663780]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "112 [D loss: 0.010021, acc.: 100.00%] [G loss: 4.833951]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "113 [D loss: 0.017245, acc.: 100.00%] [G loss: 5.134760]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "114 [D loss: 0.028723, acc.: 100.00%] [G loss: 5.226934]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "115 [D loss: 0.037882, acc.: 100.00%] [G loss: 3.837366]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "116 [D loss: 0.138543, acc.: 96.88%] [G loss: 5.565796]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "117 [D loss: 0.055388, acc.: 100.00%] [G loss: 5.301569]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "118 [D loss: 0.026928, acc.: 100.00%] [G loss: 5.246827]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "119 [D loss: 0.029079, acc.: 100.00%] [G loss: 5.702410]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "120 [D loss: 0.106486, acc.: 96.88%] [G loss: 4.439099]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "121 [D loss: 0.172195, acc.: 93.75%] [G loss: 6.899917]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "122 [D loss: 0.396178, acc.: 81.25%] [G loss: 3.932513]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "123 [D loss: 0.423727, acc.: 84.38%] [G loss: 4.624494]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "124 [D loss: 0.044635, acc.: 96.88%] [G loss: 5.950022]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "125 [D loss: 0.003071, acc.: 100.00%] [G loss: 6.010105]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "126 [D loss: 0.007320, acc.: 100.00%] [G loss: 5.328634]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "127 [D loss: 0.240821, acc.: 87.50%] [G loss: 4.349550]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "128 [D loss: 0.019630, acc.: 100.00%] [G loss: 3.875599]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "129 [D loss: 0.036859, acc.: 100.00%] [G loss: 3.981174]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "130 [D loss: 0.092995, acc.: 93.75%] [G loss: 4.902692]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "131 [D loss: 0.083204, acc.: 100.00%] [G loss: 4.371387]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "132 [D loss: 0.110102, acc.: 96.88%] [G loss: 4.472380]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "133 [D loss: 0.078236, acc.: 96.88%] [G loss: 4.598200]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "134 [D loss: 0.549042, acc.: 71.88%] [G loss: 2.992205]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "135 [D loss: 0.545504, acc.: 84.38%] [G loss: 3.256148]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "136 [D loss: 0.101862, acc.: 96.88%] [G loss: 4.473618]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "137 [D loss: 0.236825, acc.: 87.50%] [G loss: 4.190519]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "138 [D loss: 0.066258, acc.: 96.88%] [G loss: 4.472838]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "139 [D loss: 0.562970, acc.: 78.12%] [G loss: 2.086751]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "140 [D loss: 0.415358, acc.: 81.25%] [G loss: 2.233709]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "141 [D loss: 0.182053, acc.: 90.62%] [G loss: 3.648341]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "142 [D loss: 0.053346, acc.: 100.00%] [G loss: 4.227216]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "143 [D loss: 0.034274, acc.: 100.00%] [G loss: 3.725800]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "144 [D loss: 0.109216, acc.: 93.75%] [G loss: 3.118536]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "145 [D loss: 0.226570, acc.: 87.50%] [G loss: 4.195570]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "146 [D loss: 0.235540, acc.: 87.50%] [G loss: 3.381631]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "147 [D loss: 0.096461, acc.: 96.88%] [G loss: 3.434746]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "148 [D loss: 0.034610, acc.: 100.00%] [G loss: 3.301663]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "149 [D loss: 0.175932, acc.: 90.62%] [G loss: 2.889717]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "150 [D loss: 0.092080, acc.: 96.88%] [G loss: 3.381199]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "151 [D loss: 0.131159, acc.: 96.88%] [G loss: 4.280184]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "152 [D loss: 0.402807, acc.: 81.25%] [G loss: 2.592414]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "153 [D loss: 0.114847, acc.: 96.88%] [G loss: 3.166615]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "154 [D loss: 0.050050, acc.: 100.00%] [G loss: 3.733016]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "155 [D loss: 0.105224, acc.: 96.88%] [G loss: 3.442267]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "156 [D loss: 0.103913, acc.: 100.00%] [G loss: 3.887968]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "157 [D loss: 0.178804, acc.: 96.88%] [G loss: 3.720393]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "158 [D loss: 0.145311, acc.: 90.62%] [G loss: 4.617198]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "159 [D loss: 0.626081, acc.: 75.00%] [G loss: 2.614733]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "160 [D loss: 0.121360, acc.: 96.88%] [G loss: 3.878077]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "161 [D loss: 0.140783, acc.: 93.75%] [G loss: 3.519561]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "162 [D loss: 0.078805, acc.: 100.00%] [G loss: 3.135783]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "163 [D loss: 0.077633, acc.: 96.88%] [G loss: 3.552121]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "164 [D loss: 0.134977, acc.: 93.75%] [G loss: 4.231484]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "165 [D loss: 0.581968, acc.: 68.75%] [G loss: 3.064039]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "166 [D loss: 0.113419, acc.: 93.75%] [G loss: 4.487326]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "167 [D loss: 0.607697, acc.: 71.88%] [G loss: 2.111641]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "168 [D loss: 0.120581, acc.: 93.75%] [G loss: 3.023078]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "169 [D loss: 0.093918, acc.: 96.88%] [G loss: 4.175467]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "170 [D loss: 0.144882, acc.: 96.88%] [G loss: 4.469529]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "171 [D loss: 0.293585, acc.: 87.50%] [G loss: 2.458319]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "172 [D loss: 0.101472, acc.: 93.75%] [G loss: 2.852805]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "173 [D loss: 0.113036, acc.: 93.75%] [G loss: 3.704329]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "174 [D loss: 0.101201, acc.: 96.88%] [G loss: 3.075848]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "175 [D loss: 0.095803, acc.: 100.00%] [G loss: 3.758266]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "176 [D loss: 0.235805, acc.: 93.75%] [G loss: 2.931647]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "177 [D loss: 0.260729, acc.: 90.62%] [G loss: 4.367052]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "178 [D loss: 1.657762, acc.: 34.38%] [G loss: 1.054948]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "179 [D loss: 0.450156, acc.: 78.12%] [G loss: 2.187741]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "180 [D loss: 0.144911, acc.: 90.62%] [G loss: 3.820921]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "181 [D loss: 0.063729, acc.: 100.00%] [G loss: 3.947093]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "182 [D loss: 0.341503, acc.: 87.50%] [G loss: 2.786041]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "183 [D loss: 0.036740, acc.: 100.00%] [G loss: 2.894711]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "184 [D loss: 0.219939, acc.: 90.62%] [G loss: 3.693988]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "185 [D loss: 0.125373, acc.: 96.88%] [G loss: 3.247950]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "186 [D loss: 0.083523, acc.: 100.00%] [G loss: 3.440651]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "187 [D loss: 0.080938, acc.: 100.00%] [G loss: 2.699301]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "188 [D loss: 0.208153, acc.: 90.62%] [G loss: 2.806193]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "189 [D loss: 0.257885, acc.: 84.38%] [G loss: 2.668746]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "190 [D loss: 0.143661, acc.: 96.88%] [G loss: 2.923662]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "191 [D loss: 0.148099, acc.: 96.88%] [G loss: 3.779871]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "192 [D loss: 0.718525, acc.: 62.50%] [G loss: 2.011822]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "193 [D loss: 0.325245, acc.: 81.25%] [G loss: 3.991498]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "194 [D loss: 0.261466, acc.: 93.75%] [G loss: 2.521738]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "195 [D loss: 0.171724, acc.: 93.75%] [G loss: 2.992296]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "196 [D loss: 0.138924, acc.: 96.88%] [G loss: 2.878441]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "197 [D loss: 0.491810, acc.: 68.75%] [G loss: 1.510601]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "198 [D loss: 0.128612, acc.: 96.88%] [G loss: 2.349726]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "199 [D loss: 0.119488, acc.: 96.88%] [G loss: 3.383784]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "200 [D loss: 0.329179, acc.: 87.50%] [G loss: 2.181261]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "201 [D loss: 0.206342, acc.: 87.50%] [G loss: 3.620729]\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "202 [D loss: 0.272237, acc.: 87.50%] [G loss: 2.305439]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "203 [D loss: 0.198425, acc.: 93.75%] [G loss: 3.010884]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "204 [D loss: 0.182366, acc.: 93.75%] [G loss: 2.647441]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "205 [D loss: 0.400214, acc.: 75.00%] [G loss: 2.745583]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "206 [D loss: 0.236994, acc.: 93.75%] [G loss: 3.550625]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "207 [D loss: 0.647514, acc.: 65.62%] [G loss: 1.472204]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "208 [D loss: 0.189576, acc.: 90.62%] [G loss: 2.673458]\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "209 [D loss: 0.298236, acc.: 90.62%] [G loss: 3.810903]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "210 [D loss: 0.536933, acc.: 71.88%] [G loss: 1.851924]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "211 [D loss: 0.220447, acc.: 87.50%] [G loss: 3.350772]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "212 [D loss: 0.243437, acc.: 90.62%] [G loss: 3.179187]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "213 [D loss: 0.225349, acc.: 96.88%] [G loss: 2.877651]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "214 [D loss: 0.636299, acc.: 59.38%] [G loss: 2.000184]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "215 [D loss: 0.084440, acc.: 100.00%] [G loss: 3.389399]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "216 [D loss: 0.612259, acc.: 56.25%] [G loss: 1.393824]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "217 [D loss: 0.144292, acc.: 93.75%] [G loss: 2.915052]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "218 [D loss: 0.327162, acc.: 87.50%] [G loss: 2.642676]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "219 [D loss: 0.679084, acc.: 65.62%] [G loss: 2.664840]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "220 [D loss: 0.158604, acc.: 100.00%] [G loss: 2.409619]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "221 [D loss: 0.665975, acc.: 62.50%] [G loss: 2.365051]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "222 [D loss: 0.227398, acc.: 90.62%] [G loss: 2.789694]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "223 [D loss: 0.740565, acc.: 56.25%] [G loss: 1.174686]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "224 [D loss: 0.293760, acc.: 87.50%] [G loss: 2.992602]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "225 [D loss: 1.246170, acc.: 31.25%] [G loss: 0.769377]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "226 [D loss: 0.472475, acc.: 65.62%] [G loss: 2.477233]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "227 [D loss: 0.528712, acc.: 71.88%] [G loss: 2.071691]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "228 [D loss: 0.561538, acc.: 65.62%] [G loss: 2.366290]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "229 [D loss: 0.307524, acc.: 87.50%] [G loss: 2.704967]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "230 [D loss: 0.672547, acc.: 62.50%] [G loss: 1.649199]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "231 [D loss: 0.252261, acc.: 87.50%] [G loss: 3.206501]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "232 [D loss: 0.886137, acc.: 53.12%] [G loss: 1.115788]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "233 [D loss: 0.325149, acc.: 81.25%] [G loss: 2.586709]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "234 [D loss: 1.349314, acc.: 18.75%] [G loss: 0.629026]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "235 [D loss: 0.399911, acc.: 75.00%] [G loss: 2.109870]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "236 [D loss: 0.977589, acc.: 53.12%] [G loss: 1.284821]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "237 [D loss: 0.772621, acc.: 46.88%] [G loss: 1.066596]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "238 [D loss: 0.458356, acc.: 71.88%] [G loss: 1.954426]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "239 [D loss: 0.682044, acc.: 53.12%] [G loss: 1.609478]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "240 [D loss: 0.373001, acc.: 87.50%] [G loss: 1.921149]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "241 [D loss: 1.147251, acc.: 25.00%] [G loss: 0.603614]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "242 [D loss: 0.614945, acc.: 56.25%] [G loss: 1.629070]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "243 [D loss: 0.698211, acc.: 62.50%] [G loss: 1.429880]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "244 [D loss: 0.573244, acc.: 68.75%] [G loss: 1.396430]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "245 [D loss: 0.710571, acc.: 56.25%] [G loss: 1.451794]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "246 [D loss: 0.604409, acc.: 68.75%] [G loss: 1.349880]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "247 [D loss: 0.837706, acc.: 34.38%] [G loss: 1.257663]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "248 [D loss: 0.837451, acc.: 40.62%] [G loss: 1.196739]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "249 [D loss: 0.560110, acc.: 65.62%] [G loss: 1.193536]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "250 [D loss: 0.810776, acc.: 50.00%] [G loss: 0.870955]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "251 [D loss: 0.712243, acc.: 43.75%] [G loss: 1.053870]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "252 [D loss: 0.681284, acc.: 59.38%] [G loss: 1.096117]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "253 [D loss: 0.715634, acc.: 46.88%] [G loss: 1.100256]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "254 [D loss: 0.702440, acc.: 56.25%] [G loss: 1.227873]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "255 [D loss: 0.790063, acc.: 37.50%] [G loss: 0.985078]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "256 [D loss: 0.694782, acc.: 59.38%] [G loss: 0.978084]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "257 [D loss: 0.689503, acc.: 46.88%] [G loss: 1.216354]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "258 [D loss: 0.836157, acc.: 43.75%] [G loss: 0.936887]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "259 [D loss: 0.472311, acc.: 84.38%] [G loss: 1.200992]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "260 [D loss: 0.575382, acc.: 62.50%] [G loss: 1.217124]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "261 [D loss: 1.109075, acc.: 15.62%] [G loss: 0.475726]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "262 [D loss: 0.698817, acc.: 46.88%] [G loss: 0.768517]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "263 [D loss: 0.593288, acc.: 59.38%] [G loss: 1.070458]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "264 [D loss: 0.795062, acc.: 37.50%] [G loss: 0.803957]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "265 [D loss: 0.689496, acc.: 53.12%] [G loss: 0.876436]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "266 [D loss: 0.624156, acc.: 59.38%] [G loss: 1.004960]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "267 [D loss: 0.602277, acc.: 56.25%] [G loss: 1.085275]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "268 [D loss: 0.715603, acc.: 46.88%] [G loss: 0.904845]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "269 [D loss: 0.784691, acc.: 34.38%] [G loss: 0.712308]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "270 [D loss: 0.608218, acc.: 56.25%] [G loss: 0.922615]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "271 [D loss: 0.649285, acc.: 53.12%] [G loss: 0.997209]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "272 [D loss: 0.871628, acc.: 40.62%] [G loss: 0.621333]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "273 [D loss: 0.622742, acc.: 56.25%] [G loss: 0.860516]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "274 [D loss: 0.629389, acc.: 53.12%] [G loss: 0.921925]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "275 [D loss: 0.748602, acc.: 40.62%] [G loss: 0.930827]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "276 [D loss: 0.698854, acc.: 53.12%] [G loss: 0.923208]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "277 [D loss: 0.663506, acc.: 53.12%] [G loss: 0.938502]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "278 [D loss: 0.765193, acc.: 43.75%] [G loss: 0.729084]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "279 [D loss: 0.723962, acc.: 46.88%] [G loss: 0.776919]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "280 [D loss: 0.640946, acc.: 53.12%] [G loss: 0.990814]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "281 [D loss: 0.705288, acc.: 46.88%] [G loss: 0.877907]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "282 [D loss: 0.830538, acc.: 28.12%] [G loss: 0.646823]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "283 [D loss: 0.760385, acc.: 40.62%] [G loss: 0.717584]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "284 [D loss: 0.643397, acc.: 53.12%] [G loss: 0.784689]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "285 [D loss: 0.798355, acc.: 28.12%] [G loss: 0.734792]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "286 [D loss: 0.706619, acc.: 40.62%] [G loss: 0.733050]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "287 [D loss: 0.714815, acc.: 43.75%] [G loss: 0.727126]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "288 [D loss: 0.682437, acc.: 46.88%] [G loss: 0.721377]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "289 [D loss: 0.640212, acc.: 56.25%] [G loss: 0.890696]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "290 [D loss: 0.796872, acc.: 25.00%] [G loss: 0.674816]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "291 [D loss: 0.751382, acc.: 43.75%] [G loss: 0.623563]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "292 [D loss: 0.614834, acc.: 62.50%] [G loss: 0.744079]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "293 [D loss: 0.672795, acc.: 59.38%] [G loss: 0.754219]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "294 [D loss: 0.661122, acc.: 56.25%] [G loss: 0.775237]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "295 [D loss: 0.712331, acc.: 50.00%] [G loss: 0.778343]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "296 [D loss: 0.753424, acc.: 50.00%] [G loss: 0.689178]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "297 [D loss: 0.669608, acc.: 50.00%] [G loss: 0.687733]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "298 [D loss: 0.692128, acc.: 40.62%] [G loss: 0.669748]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "299 [D loss: 0.684918, acc.: 40.62%] [G loss: 0.726909]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "300 [D loss: 0.680998, acc.: 46.88%] [G loss: 0.674443]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "301 [D loss: 0.744379, acc.: 40.62%] [G loss: 0.664889]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "302 [D loss: 0.724882, acc.: 50.00%] [G loss: 0.657542]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "303 [D loss: 0.620952, acc.: 50.00%] [G loss: 0.740887]\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "304 [D loss: 0.736692, acc.: 37.50%] [G loss: 0.736895]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "305 [D loss: 0.702007, acc.: 43.75%] [G loss: 0.695586]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "306 [D loss: 0.689766, acc.: 53.12%] [G loss: 0.688654]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "307 [D loss: 0.646580, acc.: 56.25%] [G loss: 0.710530]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "308 [D loss: 0.677262, acc.: 46.88%] [G loss: 0.747309]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "309 [D loss: 0.697624, acc.: 43.75%] [G loss: 0.734855]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "310 [D loss: 0.708786, acc.: 46.88%] [G loss: 0.714907]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "311 [D loss: 0.759710, acc.: 31.25%] [G loss: 0.636917]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "312 [D loss: 0.672266, acc.: 43.75%] [G loss: 0.704417]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "313 [D loss: 0.708796, acc.: 40.62%] [G loss: 0.658648]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "314 [D loss: 0.701192, acc.: 37.50%] [G loss: 0.665786]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "315 [D loss: 0.660512, acc.: 50.00%] [G loss: 0.692017]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "316 [D loss: 0.645049, acc.: 43.75%] [G loss: 0.730090]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "317 [D loss: 0.673942, acc.: 50.00%] [G loss: 0.715231]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "318 [D loss: 0.728720, acc.: 34.38%] [G loss: 0.647518]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "319 [D loss: 0.685468, acc.: 43.75%] [G loss: 0.659546]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "320 [D loss: 0.676857, acc.: 43.75%] [G loss: 0.677740]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "321 [D loss: 0.699724, acc.: 34.38%] [G loss: 0.667278]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "322 [D loss: 0.678394, acc.: 46.88%] [G loss: 0.647238]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "323 [D loss: 0.691921, acc.: 37.50%] [G loss: 0.659586]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "324 [D loss: 0.708116, acc.: 40.62%] [G loss: 0.641093]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "325 [D loss: 0.696619, acc.: 40.62%] [G loss: 0.626816]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "326 [D loss: 0.718549, acc.: 40.62%] [G loss: 0.599154]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "327 [D loss: 0.691540, acc.: 40.62%] [G loss: 0.622979]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "328 [D loss: 0.678825, acc.: 46.88%] [G loss: 0.612069]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "329 [D loss: 0.683888, acc.: 53.12%] [G loss: 0.653485]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "330 [D loss: 0.722593, acc.: 34.38%] [G loss: 0.630056]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "331 [D loss: 0.668341, acc.: 46.88%] [G loss: 0.644016]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "332 [D loss: 0.693579, acc.: 46.88%] [G loss: 0.617172]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "333 [D loss: 0.632374, acc.: 50.00%] [G loss: 0.602926]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "334 [D loss: 0.672075, acc.: 50.00%] [G loss: 0.639081]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "335 [D loss: 0.749045, acc.: 34.38%] [G loss: 0.607175]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "336 [D loss: 0.749973, acc.: 43.75%] [G loss: 0.634403]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "337 [D loss: 0.658799, acc.: 56.25%] [G loss: 0.654628]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "338 [D loss: 0.699180, acc.: 43.75%] [G loss: 0.655196]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "339 [D loss: 0.689645, acc.: 43.75%] [G loss: 0.650474]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "340 [D loss: 0.703216, acc.: 46.88%] [G loss: 0.638437]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "341 [D loss: 0.687202, acc.: 40.62%] [G loss: 0.632587]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "342 [D loss: 0.722818, acc.: 43.75%] [G loss: 0.674269]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "343 [D loss: 0.676329, acc.: 46.88%] [G loss: 0.654577]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "344 [D loss: 0.682863, acc.: 53.12%] [G loss: 0.638335]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "345 [D loss: 0.716966, acc.: 37.50%] [G loss: 0.600382]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "346 [D loss: 0.726632, acc.: 46.88%] [G loss: 0.605933]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "347 [D loss: 0.690319, acc.: 50.00%] [G loss: 0.624468]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "348 [D loss: 0.644825, acc.: 56.25%] [G loss: 0.681554]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "349 [D loss: 0.715681, acc.: 46.88%] [G loss: 0.641421]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "350 [D loss: 0.705331, acc.: 46.88%] [G loss: 0.665687]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "351 [D loss: 0.707704, acc.: 43.75%] [G loss: 0.666121]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "352 [D loss: 0.713504, acc.: 43.75%] [G loss: 0.668170]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "353 [D loss: 0.734316, acc.: 40.62%] [G loss: 0.618883]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "354 [D loss: 0.698714, acc.: 50.00%] [G loss: 0.632083]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "355 [D loss: 0.673383, acc.: 50.00%] [G loss: 0.636626]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "356 [D loss: 0.688689, acc.: 46.88%] [G loss: 0.657249]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "357 [D loss: 0.684533, acc.: 46.88%] [G loss: 0.624197]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "358 [D loss: 0.690240, acc.: 46.88%] [G loss: 0.613960]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "359 [D loss: 0.680490, acc.: 50.00%] [G loss: 0.623101]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "360 [D loss: 0.661086, acc.: 50.00%] [G loss: 0.625463]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "361 [D loss: 0.674994, acc.: 53.12%] [G loss: 0.621581]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "362 [D loss: 0.670385, acc.: 46.88%] [G loss: 0.646385]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "363 [D loss: 0.635989, acc.: 53.12%] [G loss: 0.655821]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "364 [D loss: 0.676332, acc.: 46.88%] [G loss: 0.695325]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "365 [D loss: 0.652397, acc.: 56.25%] [G loss: 0.695770]\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "366 [D loss: 0.644463, acc.: 62.50%] [G loss: 0.673029]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "367 [D loss: 0.711224, acc.: 46.88%] [G loss: 0.661721]\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "368 [D loss: 0.657625, acc.: 62.50%] [G loss: 0.644083]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "369 [D loss: 0.700627, acc.: 53.12%] [G loss: 0.681473]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "370 [D loss: 0.673132, acc.: 56.25%] [G loss: 0.728745]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "371 [D loss: 0.668717, acc.: 53.12%] [G loss: 0.688149]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "372 [D loss: 0.678598, acc.: 53.12%] [G loss: 0.669037]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "373 [D loss: 0.688967, acc.: 46.88%] [G loss: 0.670985]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "374 [D loss: 0.645685, acc.: 59.38%] [G loss: 0.693195]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "375 [D loss: 0.710280, acc.: 40.62%] [G loss: 0.654801]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "376 [D loss: 0.656849, acc.: 50.00%] [G loss: 0.635494]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "377 [D loss: 0.704284, acc.: 46.88%] [G loss: 0.645328]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "378 [D loss: 0.649953, acc.: 50.00%] [G loss: 0.673413]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "379 [D loss: 0.669988, acc.: 50.00%] [G loss: 0.651512]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "380 [D loss: 0.675016, acc.: 46.88%] [G loss: 0.657063]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "381 [D loss: 0.675776, acc.: 53.12%] [G loss: 0.662958]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "382 [D loss: 0.698486, acc.: 43.75%] [G loss: 0.650731]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "383 [D loss: 0.664329, acc.: 43.75%] [G loss: 0.643151]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "384 [D loss: 0.672317, acc.: 43.75%] [G loss: 0.644551]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "385 [D loss: 0.674811, acc.: 46.88%] [G loss: 0.648666]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "386 [D loss: 0.682136, acc.: 43.75%] [G loss: 0.662764]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "387 [D loss: 0.687239, acc.: 50.00%] [G loss: 0.650979]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "388 [D loss: 0.666956, acc.: 50.00%] [G loss: 0.660635]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "389 [D loss: 0.665301, acc.: 46.88%] [G loss: 0.657339]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "390 [D loss: 0.688055, acc.: 46.88%] [G loss: 0.653195]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "391 [D loss: 0.645156, acc.: 53.12%] [G loss: 0.649871]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "392 [D loss: 0.658331, acc.: 50.00%] [G loss: 0.664140]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "393 [D loss: 0.648717, acc.: 53.12%] [G loss: 0.668635]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "394 [D loss: 0.660313, acc.: 50.00%] [G loss: 0.664909]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "395 [D loss: 0.650702, acc.: 53.12%] [G loss: 0.630153]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "396 [D loss: 0.635932, acc.: 50.00%] [G loss: 0.645688]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "397 [D loss: 0.689932, acc.: 40.62%] [G loss: 0.637396]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "398 [D loss: 0.659342, acc.: 50.00%] [G loss: 0.641403]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "399 [D loss: 0.648672, acc.: 50.00%] [G loss: 0.644639]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "400 [D loss: 0.663396, acc.: 50.00%] [G loss: 0.672543]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "401 [D loss: 0.648147, acc.: 59.38%] [G loss: 0.673691]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "402 [D loss: 0.647521, acc.: 50.00%] [G loss: 0.671422]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "403 [D loss: 0.621809, acc.: 56.25%] [G loss: 0.676930]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "404 [D loss: 0.661842, acc.: 53.12%] [G loss: 0.670141]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "405 [D loss: 0.662823, acc.: 43.75%] [G loss: 0.680942]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "406 [D loss: 0.639541, acc.: 53.12%] [G loss: 0.673054]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "407 [D loss: 0.622337, acc.: 53.12%] [G loss: 0.688151]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "408 [D loss: 0.641600, acc.: 50.00%] [G loss: 0.683222]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "409 [D loss: 0.658401, acc.: 43.75%] [G loss: 0.667558]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "410 [D loss: 0.621202, acc.: 53.12%] [G loss: 0.688814]\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "411 [D loss: 0.653153, acc.: 50.00%] [G loss: 0.699234]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "412 [D loss: 0.632634, acc.: 46.88%] [G loss: 0.701823]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "413 [D loss: 0.654149, acc.: 43.75%] [G loss: 0.693578]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "414 [D loss: 0.622384, acc.: 53.12%] [G loss: 0.703680]\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "415 [D loss: 0.657307, acc.: 46.88%] [G loss: 0.698157]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "416 [D loss: 0.643947, acc.: 50.00%] [G loss: 0.673186]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "417 [D loss: 0.663961, acc.: 50.00%] [G loss: 0.687603]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "418 [D loss: 0.623707, acc.: 50.00%] [G loss: 0.671385]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "419 [D loss: 0.620494, acc.: 53.12%] [G loss: 0.668100]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "420 [D loss: 0.661859, acc.: 50.00%] [G loss: 0.665941]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "421 [D loss: 0.677627, acc.: 50.00%] [G loss: 0.678492]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "422 [D loss: 0.682230, acc.: 46.88%] [G loss: 0.729928]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "423 [D loss: 0.655142, acc.: 50.00%] [G loss: 0.719566]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "424 [D loss: 0.669342, acc.: 50.00%] [G loss: 0.710375]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "425 [D loss: 0.657814, acc.: 53.12%] [G loss: 0.674275]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "426 [D loss: 0.643385, acc.: 50.00%] [G loss: 0.675014]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "427 [D loss: 0.619722, acc.: 53.12%] [G loss: 0.642050]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "428 [D loss: 0.629404, acc.: 53.12%] [G loss: 0.666490]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "429 [D loss: 0.644199, acc.: 59.38%] [G loss: 0.669681]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "430 [D loss: 0.627903, acc.: 62.50%] [G loss: 0.680539]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "431 [D loss: 0.637402, acc.: 53.12%] [G loss: 0.676789]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "432 [D loss: 0.657507, acc.: 46.88%] [G loss: 0.695990]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "433 [D loss: 0.636302, acc.: 50.00%] [G loss: 0.683493]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "434 [D loss: 0.648202, acc.: 59.38%] [G loss: 0.695694]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "435 [D loss: 0.620588, acc.: 50.00%] [G loss: 0.733643]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "436 [D loss: 0.631380, acc.: 50.00%] [G loss: 0.744460]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "437 [D loss: 0.647965, acc.: 50.00%] [G loss: 0.708596]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "438 [D loss: 0.677911, acc.: 46.88%] [G loss: 0.690030]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "439 [D loss: 0.655656, acc.: 53.12%] [G loss: 0.713943]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "440 [D loss: 0.645581, acc.: 56.25%] [G loss: 0.729144]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "441 [D loss: 0.611793, acc.: 53.12%] [G loss: 0.723365]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "442 [D loss: 0.635924, acc.: 56.25%] [G loss: 0.709002]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "443 [D loss: 0.616733, acc.: 62.50%] [G loss: 0.713625]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "444 [D loss: 0.637112, acc.: 62.50%] [G loss: 0.711301]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "445 [D loss: 0.608593, acc.: 59.38%] [G loss: 0.713631]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "446 [D loss: 0.648336, acc.: 56.25%] [G loss: 0.709300]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "447 [D loss: 0.647953, acc.: 46.88%] [G loss: 0.706213]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "448 [D loss: 0.637295, acc.: 56.25%] [G loss: 0.699470]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "449 [D loss: 0.638639, acc.: 50.00%] [G loss: 0.671280]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "450 [D loss: 0.615885, acc.: 56.25%] [G loss: 0.706489]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "451 [D loss: 0.680760, acc.: 53.12%] [G loss: 0.703937]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "452 [D loss: 0.658884, acc.: 50.00%] [G loss: 0.717524]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "453 [D loss: 0.648915, acc.: 46.88%] [G loss: 0.691980]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "454 [D loss: 0.642452, acc.: 53.12%] [G loss: 0.682182]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "455 [D loss: 0.638820, acc.: 59.38%] [G loss: 0.684080]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "456 [D loss: 0.670798, acc.: 56.25%] [G loss: 0.690425]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "457 [D loss: 0.631977, acc.: 59.38%] [G loss: 0.712735]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "458 [D loss: 0.627954, acc.: 62.50%] [G loss: 0.714547]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "459 [D loss: 0.666418, acc.: 56.25%] [G loss: 0.711593]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "460 [D loss: 0.641381, acc.: 62.50%] [G loss: 0.674425]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "461 [D loss: 0.671609, acc.: 56.25%] [G loss: 0.705551]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "462 [D loss: 0.616631, acc.: 62.50%] [G loss: 0.731680]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "463 [D loss: 0.675700, acc.: 46.88%] [G loss: 0.714179]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "464 [D loss: 0.684939, acc.: 50.00%] [G loss: 0.719123]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "465 [D loss: 0.649300, acc.: 62.50%] [G loss: 0.697488]\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "466 [D loss: 0.655729, acc.: 62.50%] [G loss: 0.684546]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "467 [D loss: 0.642228, acc.: 62.50%] [G loss: 0.670945]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "468 [D loss: 0.620197, acc.: 59.38%] [G loss: 0.708818]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "469 [D loss: 0.705728, acc.: 46.88%] [G loss: 0.706283]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "470 [D loss: 0.660515, acc.: 59.38%] [G loss: 0.706516]\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "471 [D loss: 0.656092, acc.: 53.12%] [G loss: 0.725535]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "472 [D loss: 0.670299, acc.: 56.25%] [G loss: 0.733677]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "473 [D loss: 0.671527, acc.: 59.38%] [G loss: 0.746356]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "474 [D loss: 0.645123, acc.: 59.38%] [G loss: 0.735893]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "475 [D loss: 0.669592, acc.: 50.00%] [G loss: 0.736785]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "476 [D loss: 0.651220, acc.: 59.38%] [G loss: 0.719106]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "477 [D loss: 0.653983, acc.: 56.25%] [G loss: 0.729196]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "478 [D loss: 0.656737, acc.: 50.00%] [G loss: 0.750721]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "479 [D loss: 0.660880, acc.: 50.00%] [G loss: 0.703057]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "480 [D loss: 0.669410, acc.: 59.38%] [G loss: 0.673723]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "481 [D loss: 0.627668, acc.: 62.50%] [G loss: 0.687468]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "482 [D loss: 0.676767, acc.: 65.62%] [G loss: 0.722079]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "483 [D loss: 0.635113, acc.: 59.38%] [G loss: 0.743015]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "484 [D loss: 0.670012, acc.: 53.12%] [G loss: 0.774145]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "485 [D loss: 0.665090, acc.: 56.25%] [G loss: 0.760359]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "486 [D loss: 0.638910, acc.: 62.50%] [G loss: 0.760414]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "487 [D loss: 0.688147, acc.: 43.75%] [G loss: 0.688258]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "488 [D loss: 0.681070, acc.: 46.88%] [G loss: 0.711417]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "489 [D loss: 0.669193, acc.: 43.75%] [G loss: 0.717698]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "490 [D loss: 0.577419, acc.: 65.62%] [G loss: 0.735966]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "491 [D loss: 0.667583, acc.: 50.00%] [G loss: 0.718030]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "492 [D loss: 0.627878, acc.: 62.50%] [G loss: 0.701080]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "493 [D loss: 0.654474, acc.: 59.38%] [G loss: 0.682973]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "494 [D loss: 0.623240, acc.: 56.25%] [G loss: 0.683765]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "495 [D loss: 0.641765, acc.: 68.75%] [G loss: 0.721168]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "496 [D loss: 0.673815, acc.: 65.62%] [G loss: 0.728915]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "497 [D loss: 0.660100, acc.: 65.62%] [G loss: 0.738846]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "498 [D loss: 0.637048, acc.: 56.25%] [G loss: 0.744187]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "499 [D loss: 0.678360, acc.: 53.12%] [G loss: 0.716913]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "500 [D loss: 0.618321, acc.: 65.62%] [G loss: 0.687722]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "501 [D loss: 0.639200, acc.: 56.25%] [G loss: 0.706810]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "502 [D loss: 0.617716, acc.: 68.75%] [G loss: 0.732227]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "503 [D loss: 0.612348, acc.: 71.88%] [G loss: 0.723383]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "504 [D loss: 0.659103, acc.: 50.00%] [G loss: 0.741054]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "505 [D loss: 0.606608, acc.: 65.62%] [G loss: 0.759309]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "506 [D loss: 0.690917, acc.: 50.00%] [G loss: 0.760574]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "507 [D loss: 0.660438, acc.: 46.88%] [G loss: 0.766900]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "508 [D loss: 0.657837, acc.: 59.38%] [G loss: 0.733458]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "509 [D loss: 0.666099, acc.: 56.25%] [G loss: 0.719580]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "510 [D loss: 0.641998, acc.: 59.38%] [G loss: 0.706844]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "511 [D loss: 0.641895, acc.: 53.12%] [G loss: 0.696028]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "512 [D loss: 0.626660, acc.: 56.25%] [G loss: 0.692906]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "513 [D loss: 0.647425, acc.: 43.75%] [G loss: 0.705134]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "514 [D loss: 0.651620, acc.: 53.12%] [G loss: 0.704290]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "515 [D loss: 0.643964, acc.: 53.12%] [G loss: 0.734276]\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "516 [D loss: 0.619922, acc.: 50.00%] [G loss: 0.727828]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "517 [D loss: 0.630368, acc.: 65.62%] [G loss: 0.733928]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "518 [D loss: 0.613972, acc.: 62.50%] [G loss: 0.720002]\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "519 [D loss: 0.675384, acc.: 59.38%] [G loss: 0.700005]\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "520 [D loss: 0.604656, acc.: 62.50%] [G loss: 0.706852]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "521 [D loss: 0.641597, acc.: 62.50%] [G loss: 0.694567]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "522 [D loss: 0.635819, acc.: 53.12%] [G loss: 0.711198]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "523 [D loss: 0.646484, acc.: 56.25%] [G loss: 0.714726]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "524 [D loss: 0.624724, acc.: 53.12%] [G loss: 0.697942]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "525 [D loss: 0.671169, acc.: 59.38%] [G loss: 0.669832]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "526 [D loss: 0.592903, acc.: 68.75%] [G loss: 0.663557]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "527 [D loss: 0.681167, acc.: 50.00%] [G loss: 0.678940]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "528 [D loss: 0.583179, acc.: 75.00%] [G loss: 0.699634]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "529 [D loss: 0.619248, acc.: 53.12%] [G loss: 0.690784]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "530 [D loss: 0.647865, acc.: 65.62%] [G loss: 0.709263]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "531 [D loss: 0.670812, acc.: 59.38%] [G loss: 0.706958]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "532 [D loss: 0.656691, acc.: 62.50%] [G loss: 0.727250]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "533 [D loss: 0.628381, acc.: 59.38%] [G loss: 0.749128]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "534 [D loss: 0.619298, acc.: 65.62%] [G loss: 0.737428]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "535 [D loss: 0.659633, acc.: 59.38%] [G loss: 0.709930]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "536 [D loss: 0.630116, acc.: 68.75%] [G loss: 0.775317]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "537 [D loss: 0.659608, acc.: 56.25%] [G loss: 0.738819]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "538 [D loss: 0.742297, acc.: 50.00%] [G loss: 0.699232]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "539 [D loss: 0.667605, acc.: 56.25%] [G loss: 0.730735]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "540 [D loss: 0.694900, acc.: 56.25%] [G loss: 0.739172]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "541 [D loss: 0.630926, acc.: 56.25%] [G loss: 0.757905]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "542 [D loss: 0.617034, acc.: 65.62%] [G loss: 0.842557]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "543 [D loss: 0.654672, acc.: 75.00%] [G loss: 0.815947]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "544 [D loss: 0.719663, acc.: 37.50%] [G loss: 0.716511]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "545 [D loss: 0.675004, acc.: 50.00%] [G loss: 0.695965]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "546 [D loss: 0.667321, acc.: 56.25%] [G loss: 0.712181]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "547 [D loss: 0.716135, acc.: 43.75%] [G loss: 0.694365]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "548 [D loss: 0.673252, acc.: 46.88%] [G loss: 0.719270]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "549 [D loss: 0.658734, acc.: 40.62%] [G loss: 0.766337]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "550 [D loss: 0.673134, acc.: 56.25%] [G loss: 0.748301]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "551 [D loss: 0.718445, acc.: 34.38%] [G loss: 0.709368]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "552 [D loss: 0.685025, acc.: 59.38%] [G loss: 0.698182]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "553 [D loss: 0.701735, acc.: 50.00%] [G loss: 0.700678]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "554 [D loss: 0.654231, acc.: 62.50%] [G loss: 0.710053]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "555 [D loss: 0.633828, acc.: 62.50%] [G loss: 0.754298]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "556 [D loss: 0.656540, acc.: 53.12%] [G loss: 0.740610]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "557 [D loss: 0.660139, acc.: 56.25%] [G loss: 0.746643]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "558 [D loss: 0.705606, acc.: 37.50%] [G loss: 0.711061]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "559 [D loss: 0.651712, acc.: 56.25%] [G loss: 0.703149]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "560 [D loss: 0.634495, acc.: 53.12%] [G loss: 0.709998]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "561 [D loss: 0.646538, acc.: 65.62%] [G loss: 0.721789]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "562 [D loss: 0.643627, acc.: 59.38%] [G loss: 0.763198]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "563 [D loss: 0.638620, acc.: 71.88%] [G loss: 0.772521]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "564 [D loss: 0.653534, acc.: 59.38%] [G loss: 0.758265]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "565 [D loss: 0.652563, acc.: 62.50%] [G loss: 0.753531]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "566 [D loss: 0.661598, acc.: 59.38%] [G loss: 0.753880]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "567 [D loss: 0.649725, acc.: 62.50%] [G loss: 0.747356]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "568 [D loss: 0.662661, acc.: 53.12%] [G loss: 0.716928]\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "569 [D loss: 0.690631, acc.: 53.12%] [G loss: 0.713755]\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "570 [D loss: 0.650609, acc.: 53.12%] [G loss: 0.716015]\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "571 [D loss: 0.646525, acc.: 53.12%] [G loss: 0.708444]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "572 [D loss: 0.630747, acc.: 65.62%] [G loss: 0.710834]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "573 [D loss: 0.686093, acc.: 50.00%] [G loss: 0.715954]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "574 [D loss: 0.680662, acc.: 59.38%] [G loss: 0.713284]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "575 [D loss: 0.690398, acc.: 46.88%] [G loss: 0.682097]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "576 [D loss: 0.642793, acc.: 56.25%] [G loss: 0.679699]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "577 [D loss: 0.661392, acc.: 56.25%] [G loss: 0.686588]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "578 [D loss: 0.673040, acc.: 53.12%] [G loss: 0.664828]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "579 [D loss: 0.646443, acc.: 50.00%] [G loss: 0.679757]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "580 [D loss: 0.656125, acc.: 59.38%] [G loss: 0.724104]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "581 [D loss: 0.641094, acc.: 59.38%] [G loss: 0.753387]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "582 [D loss: 0.666910, acc.: 65.62%] [G loss: 0.802833]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "583 [D loss: 0.669100, acc.: 56.25%] [G loss: 0.755703]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "584 [D loss: 0.701268, acc.: 43.75%] [G loss: 0.722081]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "585 [D loss: 0.642927, acc.: 62.50%] [G loss: 0.699539]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "586 [D loss: 0.667691, acc.: 53.12%] [G loss: 0.691217]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "587 [D loss: 0.625625, acc.: 65.62%] [G loss: 0.733980]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "588 [D loss: 0.677386, acc.: 43.75%] [G loss: 0.760967]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "589 [D loss: 0.655764, acc.: 56.25%] [G loss: 0.734363]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "590 [D loss: 0.610219, acc.: 68.75%] [G loss: 0.785237]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "591 [D loss: 0.651179, acc.: 62.50%] [G loss: 0.757560]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "592 [D loss: 0.632716, acc.: 68.75%] [G loss: 0.733755]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "593 [D loss: 0.669352, acc.: 56.25%] [G loss: 0.723038]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "594 [D loss: 0.682386, acc.: 62.50%] [G loss: 0.720136]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "595 [D loss: 0.660633, acc.: 59.38%] [G loss: 0.719843]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "596 [D loss: 0.675349, acc.: 62.50%] [G loss: 0.698571]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "597 [D loss: 0.642589, acc.: 53.12%] [G loss: 0.692323]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "598 [D loss: 0.631721, acc.: 68.75%] [G loss: 0.686625]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "599 [D loss: 0.650359, acc.: 71.88%] [G loss: 0.678568]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "600 [D loss: 0.663676, acc.: 59.38%] [G loss: 0.683386]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "601 [D loss: 0.590657, acc.: 78.12%] [G loss: 0.683617]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "602 [D loss: 0.696278, acc.: 71.88%] [G loss: 0.694864]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "603 [D loss: 0.659324, acc.: 59.38%] [G loss: 0.753592]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "604 [D loss: 0.688036, acc.: 53.12%] [G loss: 0.740357]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "605 [D loss: 0.661387, acc.: 71.88%] [G loss: 0.770150]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "606 [D loss: 0.638300, acc.: 65.62%] [G loss: 0.757076]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "607 [D loss: 0.688522, acc.: 43.75%] [G loss: 0.717533]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "608 [D loss: 0.630230, acc.: 68.75%] [G loss: 0.728983]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "609 [D loss: 0.655720, acc.: 59.38%] [G loss: 0.766549]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "610 [D loss: 0.633532, acc.: 65.62%] [G loss: 0.768737]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "611 [D loss: 0.655720, acc.: 56.25%] [G loss: 0.703129]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "612 [D loss: 0.639602, acc.: 68.75%] [G loss: 0.723211]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "613 [D loss: 0.682730, acc.: 43.75%] [G loss: 0.752047]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "614 [D loss: 0.680427, acc.: 53.12%] [G loss: 0.713848]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "615 [D loss: 0.656160, acc.: 53.12%] [G loss: 0.724569]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "616 [D loss: 0.640054, acc.: 65.62%] [G loss: 0.729915]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "617 [D loss: 0.636412, acc.: 59.38%] [G loss: 0.700909]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "618 [D loss: 0.640931, acc.: 53.12%] [G loss: 0.723032]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "619 [D loss: 0.674815, acc.: 50.00%] [G loss: 0.715358]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "620 [D loss: 0.664013, acc.: 56.25%] [G loss: 0.747201]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "621 [D loss: 0.639146, acc.: 56.25%] [G loss: 0.740617]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "622 [D loss: 0.667049, acc.: 53.12%] [G loss: 0.741309]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "623 [D loss: 0.637135, acc.: 59.38%] [G loss: 0.712397]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "624 [D loss: 0.626301, acc.: 59.38%] [G loss: 0.700116]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "625 [D loss: 0.666112, acc.: 62.50%] [G loss: 0.710861]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "626 [D loss: 0.620173, acc.: 62.50%] [G loss: 0.724893]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "627 [D loss: 0.607431, acc.: 71.88%] [G loss: 0.752156]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "628 [D loss: 0.629117, acc.: 53.12%] [G loss: 0.769954]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "629 [D loss: 0.643844, acc.: 59.38%] [G loss: 0.769735]\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "630 [D loss: 0.695458, acc.: 50.00%] [G loss: 0.737459]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "631 [D loss: 0.656531, acc.: 59.38%] [G loss: 0.743307]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "632 [D loss: 0.662931, acc.: 53.12%] [G loss: 0.698537]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "633 [D loss: 0.641447, acc.: 65.62%] [G loss: 0.675279]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "634 [D loss: 0.568628, acc.: 68.75%] [G loss: 0.693569]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "635 [D loss: 0.673019, acc.: 59.38%] [G loss: 0.694781]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "636 [D loss: 0.669539, acc.: 53.12%] [G loss: 0.705052]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "637 [D loss: 0.673570, acc.: 65.62%] [G loss: 0.711653]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "638 [D loss: 0.666182, acc.: 53.12%] [G loss: 0.757827]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "639 [D loss: 0.648682, acc.: 53.12%] [G loss: 0.731617]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "640 [D loss: 0.675122, acc.: 46.88%] [G loss: 0.754743]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "641 [D loss: 0.632437, acc.: 59.38%] [G loss: 0.745757]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "642 [D loss: 0.661282, acc.: 71.88%] [G loss: 0.753450]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "643 [D loss: 0.678062, acc.: 46.88%] [G loss: 0.732452]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "644 [D loss: 0.628751, acc.: 71.88%] [G loss: 0.766594]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "645 [D loss: 0.670511, acc.: 62.50%] [G loss: 0.756563]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "646 [D loss: 0.613935, acc.: 75.00%] [G loss: 0.762804]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "647 [D loss: 0.672799, acc.: 56.25%] [G loss: 0.738978]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "648 [D loss: 0.694187, acc.: 53.12%] [G loss: 0.715857]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "649 [D loss: 0.656849, acc.: 53.12%] [G loss: 0.699413]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "650 [D loss: 0.678509, acc.: 53.12%] [G loss: 0.763928]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "651 [D loss: 0.646897, acc.: 75.00%] [G loss: 0.732866]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "652 [D loss: 0.635358, acc.: 68.75%] [G loss: 0.732787]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "653 [D loss: 0.660589, acc.: 62.50%] [G loss: 0.702377]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "654 [D loss: 0.672025, acc.: 56.25%] [G loss: 0.682250]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "655 [D loss: 0.612421, acc.: 62.50%] [G loss: 0.701956]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "656 [D loss: 0.611251, acc.: 75.00%] [G loss: 0.672279]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "657 [D loss: 0.659106, acc.: 62.50%] [G loss: 0.679115]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "658 [D loss: 0.647776, acc.: 62.50%] [G loss: 0.706825]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "659 [D loss: 0.670504, acc.: 59.38%] [G loss: 0.747130]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "660 [D loss: 0.643044, acc.: 59.38%] [G loss: 0.751159]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "661 [D loss: 0.651851, acc.: 68.75%] [G loss: 0.754444]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "662 [D loss: 0.601137, acc.: 75.00%] [G loss: 0.768639]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "663 [D loss: 0.673652, acc.: 53.12%] [G loss: 0.753627]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "664 [D loss: 0.650488, acc.: 62.50%] [G loss: 0.746312]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "665 [D loss: 0.591886, acc.: 71.88%] [G loss: 0.772236]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "666 [D loss: 0.636644, acc.: 75.00%] [G loss: 0.708720]\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "667 [D loss: 0.740736, acc.: 59.38%] [G loss: 0.717204]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "668 [D loss: 0.622471, acc.: 68.75%] [G loss: 0.727285]\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "669 [D loss: 0.694477, acc.: 53.12%] [G loss: 0.761413]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "670 [D loss: 0.670985, acc.: 56.25%] [G loss: 0.791059]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "671 [D loss: 0.650399, acc.: 59.38%] [G loss: 0.784732]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "672 [D loss: 0.620162, acc.: 65.62%] [G loss: 0.749110]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "673 [D loss: 0.635406, acc.: 68.75%] [G loss: 0.749268]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "674 [D loss: 0.686721, acc.: 53.12%] [G loss: 0.740669]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "675 [D loss: 0.625069, acc.: 62.50%] [G loss: 0.767492]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "676 [D loss: 0.619382, acc.: 56.25%] [G loss: 0.791192]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "677 [D loss: 0.668244, acc.: 56.25%] [G loss: 0.794926]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "678 [D loss: 0.657160, acc.: 56.25%] [G loss: 0.748224]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "679 [D loss: 0.621044, acc.: 62.50%] [G loss: 0.792695]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "680 [D loss: 0.657297, acc.: 53.12%] [G loss: 0.722529]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "681 [D loss: 0.646633, acc.: 71.88%] [G loss: 0.707409]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "682 [D loss: 0.679691, acc.: 56.25%] [G loss: 0.715970]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "683 [D loss: 0.649233, acc.: 56.25%] [G loss: 0.703375]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "684 [D loss: 0.651490, acc.: 56.25%] [G loss: 0.723089]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "685 [D loss: 0.620030, acc.: 68.75%] [G loss: 0.732554]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "686 [D loss: 0.685794, acc.: 50.00%] [G loss: 0.751805]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "687 [D loss: 0.647493, acc.: 59.38%] [G loss: 0.744469]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "688 [D loss: 0.609588, acc.: 59.38%] [G loss: 0.762738]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "689 [D loss: 0.617752, acc.: 71.88%] [G loss: 0.749968]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "690 [D loss: 0.610285, acc.: 71.88%] [G loss: 0.748556]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "691 [D loss: 0.602558, acc.: 68.75%] [G loss: 0.771680]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "692 [D loss: 0.711633, acc.: 62.50%] [G loss: 0.755061]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "693 [D loss: 0.674335, acc.: 56.25%] [G loss: 0.798677]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "694 [D loss: 0.604286, acc.: 75.00%] [G loss: 0.816630]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "695 [D loss: 0.661239, acc.: 59.38%] [G loss: 0.793693]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "696 [D loss: 0.636505, acc.: 71.88%] [G loss: 0.774020]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "697 [D loss: 0.648982, acc.: 62.50%] [G loss: 0.750641]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "698 [D loss: 0.630410, acc.: 59.38%] [G loss: 0.726537]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "699 [D loss: 0.683362, acc.: 50.00%] [G loss: 0.705859]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "700 [D loss: 0.628701, acc.: 53.12%] [G loss: 0.736780]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "701 [D loss: 0.617712, acc.: 65.62%] [G loss: 0.756186]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "702 [D loss: 0.701358, acc.: 46.88%] [G loss: 0.730104]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "703 [D loss: 0.620313, acc.: 65.62%] [G loss: 0.759007]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "704 [D loss: 0.612700, acc.: 68.75%] [G loss: 0.771794]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "705 [D loss: 0.655771, acc.: 56.25%] [G loss: 0.767756]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "706 [D loss: 0.592767, acc.: 81.25%] [G loss: 0.797370]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "707 [D loss: 0.643571, acc.: 65.62%] [G loss: 0.785971]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "708 [D loss: 0.646346, acc.: 62.50%] [G loss: 0.759842]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "709 [D loss: 0.626871, acc.: 68.75%] [G loss: 0.748519]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "710 [D loss: 0.580994, acc.: 68.75%] [G loss: 0.737056]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "711 [D loss: 0.618106, acc.: 59.38%] [G loss: 0.753838]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "712 [D loss: 0.697709, acc.: 43.75%] [G loss: 0.740708]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "713 [D loss: 0.622550, acc.: 65.62%] [G loss: 0.779694]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "714 [D loss: 0.577927, acc.: 71.88%] [G loss: 0.782577]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "715 [D loss: 0.643116, acc.: 59.38%] [G loss: 0.756147]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "716 [D loss: 0.671058, acc.: 56.25%] [G loss: 0.793649]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "717 [D loss: 0.630633, acc.: 65.62%] [G loss: 0.779892]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "718 [D loss: 0.613252, acc.: 71.88%] [G loss: 0.793093]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "719 [D loss: 0.629900, acc.: 62.50%] [G loss: 0.805105]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "720 [D loss: 0.648621, acc.: 65.62%] [G loss: 0.822860]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "721 [D loss: 0.664803, acc.: 62.50%] [G loss: 0.747148]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "722 [D loss: 0.634735, acc.: 59.38%] [G loss: 0.732534]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "723 [D loss: 0.631352, acc.: 59.38%] [G loss: 0.721069]\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "724 [D loss: 0.588675, acc.: 71.88%] [G loss: 0.769251]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "725 [D loss: 0.654457, acc.: 56.25%] [G loss: 0.776088]\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "726 [D loss: 0.652529, acc.: 56.25%] [G loss: 0.769802]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "727 [D loss: 0.530908, acc.: 87.50%] [G loss: 0.820542]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "728 [D loss: 0.605425, acc.: 65.62%] [G loss: 0.782814]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "729 [D loss: 0.647298, acc.: 68.75%] [G loss: 0.736200]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "730 [D loss: 0.639965, acc.: 65.62%] [G loss: 0.733440]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "731 [D loss: 0.632718, acc.: 56.25%] [G loss: 0.722631]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "732 [D loss: 0.606435, acc.: 65.62%] [G loss: 0.781283]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "733 [D loss: 0.624920, acc.: 71.88%] [G loss: 0.787198]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "734 [D loss: 0.627638, acc.: 78.12%] [G loss: 0.801461]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "735 [D loss: 0.689338, acc.: 65.62%] [G loss: 0.856642]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "736 [D loss: 0.698260, acc.: 53.12%] [G loss: 0.795986]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "737 [D loss: 0.694198, acc.: 53.12%] [G loss: 0.766314]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "738 [D loss: 0.640042, acc.: 56.25%] [G loss: 0.752875]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "739 [D loss: 0.684466, acc.: 53.12%] [G loss: 0.696460]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "740 [D loss: 0.661360, acc.: 56.25%] [G loss: 0.685543]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "741 [D loss: 0.649560, acc.: 56.25%] [G loss: 0.728932]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "742 [D loss: 0.674620, acc.: 59.38%] [G loss: 0.781176]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "743 [D loss: 0.689609, acc.: 43.75%] [G loss: 0.792896]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "744 [D loss: 0.664144, acc.: 65.62%] [G loss: 0.790883]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "745 [D loss: 0.668554, acc.: 56.25%] [G loss: 0.802373]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "746 [D loss: 0.675277, acc.: 40.62%] [G loss: 0.870394]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "747 [D loss: 0.711131, acc.: 43.75%] [G loss: 0.805089]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "748 [D loss: 0.663568, acc.: 53.12%] [G loss: 0.764984]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "749 [D loss: 0.653165, acc.: 53.12%] [G loss: 0.758972]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "750 [D loss: 0.670336, acc.: 43.75%] [G loss: 0.783718]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "751 [D loss: 0.661438, acc.: 46.88%] [G loss: 0.807719]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "752 [D loss: 0.642159, acc.: 65.62%] [G loss: 0.815936]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "753 [D loss: 0.643883, acc.: 59.38%] [G loss: 0.775931]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "754 [D loss: 0.653858, acc.: 65.62%] [G loss: 0.776822]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "755 [D loss: 0.649587, acc.: 65.62%] [G loss: 0.763805]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "756 [D loss: 0.649278, acc.: 56.25%] [G loss: 0.753298]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "757 [D loss: 0.627078, acc.: 68.75%] [G loss: 0.782235]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "758 [D loss: 0.608271, acc.: 71.88%] [G loss: 0.823704]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "759 [D loss: 0.610085, acc.: 65.62%] [G loss: 0.823699]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "760 [D loss: 0.598448, acc.: 78.12%] [G loss: 0.813307]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "761 [D loss: 0.642509, acc.: 62.50%] [G loss: 0.824599]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "762 [D loss: 0.624973, acc.: 62.50%] [G loss: 0.775562]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "763 [D loss: 0.580909, acc.: 71.88%] [G loss: 0.802795]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "764 [D loss: 0.636565, acc.: 68.75%] [G loss: 0.782136]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "765 [D loss: 0.598625, acc.: 75.00%] [G loss: 0.778722]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "766 [D loss: 0.632880, acc.: 59.38%] [G loss: 0.765721]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "767 [D loss: 0.614384, acc.: 71.88%] [G loss: 0.823870]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "768 [D loss: 0.642108, acc.: 71.88%] [G loss: 0.833846]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "769 [D loss: 0.633779, acc.: 68.75%] [G loss: 0.794115]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "770 [D loss: 0.616305, acc.: 65.62%] [G loss: 0.789675]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "771 [D loss: 0.618101, acc.: 75.00%] [G loss: 0.760526]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "772 [D loss: 0.639068, acc.: 62.50%] [G loss: 0.756221]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "773 [D loss: 0.642110, acc.: 59.38%] [G loss: 0.744384]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "774 [D loss: 0.620310, acc.: 71.88%] [G loss: 0.723237]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "775 [D loss: 0.638093, acc.: 62.50%] [G loss: 0.748947]\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "776 [D loss: 0.648102, acc.: 56.25%] [G loss: 0.778929]\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "777 [D loss: 0.616932, acc.: 65.62%] [G loss: 0.788370]\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "778 [D loss: 0.623981, acc.: 59.38%] [G loss: 0.804833]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "779 [D loss: 0.600839, acc.: 75.00%] [G loss: 0.806745]\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "780 [D loss: 0.663989, acc.: 65.62%] [G loss: 0.783059]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "781 [D loss: 0.646147, acc.: 62.50%] [G loss: 0.780738]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "782 [D loss: 0.634575, acc.: 59.38%] [G loss: 0.779964]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "783 [D loss: 0.611973, acc.: 84.38%] [G loss: 0.802639]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "784 [D loss: 0.637612, acc.: 65.62%] [G loss: 0.783532]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "785 [D loss: 0.619674, acc.: 59.38%] [G loss: 0.772447]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "786 [D loss: 0.640997, acc.: 65.62%] [G loss: 0.805891]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "787 [D loss: 0.648466, acc.: 65.62%] [G loss: 0.798859]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "788 [D loss: 0.632978, acc.: 65.62%] [G loss: 0.811360]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "789 [D loss: 0.640948, acc.: 62.50%] [G loss: 0.850987]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "790 [D loss: 0.702155, acc.: 56.25%] [G loss: 0.829888]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "791 [D loss: 0.648598, acc.: 56.25%] [G loss: 0.797006]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "792 [D loss: 0.701251, acc.: 31.25%] [G loss: 0.754048]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "793 [D loss: 0.631087, acc.: 59.38%] [G loss: 0.731221]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "794 [D loss: 0.634060, acc.: 65.62%] [G loss: 0.749281]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "795 [D loss: 0.627271, acc.: 65.62%] [G loss: 0.759175]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "796 [D loss: 0.634376, acc.: 68.75%] [G loss: 0.725038]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "797 [D loss: 0.631317, acc.: 68.75%] [G loss: 0.724039]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "798 [D loss: 0.620057, acc.: 71.88%] [G loss: 0.726819]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "799 [D loss: 0.657793, acc.: 50.00%] [G loss: 0.757692]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "800 [D loss: 0.694555, acc.: 53.12%] [G loss: 0.740382]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "801 [D loss: 0.656439, acc.: 62.50%] [G loss: 0.779566]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "802 [D loss: 0.582913, acc.: 65.62%] [G loss: 0.775148]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "803 [D loss: 0.602258, acc.: 75.00%] [G loss: 0.758525]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "804 [D loss: 0.593490, acc.: 56.25%] [G loss: 0.814136]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "805 [D loss: 0.597673, acc.: 71.88%] [G loss: 0.847063]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "806 [D loss: 0.644983, acc.: 59.38%] [G loss: 0.814733]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "807 [D loss: 0.654234, acc.: 56.25%] [G loss: 0.782275]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "808 [D loss: 0.664524, acc.: 53.12%] [G loss: 0.767661]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "809 [D loss: 0.669416, acc.: 56.25%] [G loss: 0.774126]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "810 [D loss: 0.642148, acc.: 53.12%] [G loss: 0.806040]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "811 [D loss: 0.614558, acc.: 65.62%] [G loss: 0.817560]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "812 [D loss: 0.650090, acc.: 65.62%] [G loss: 0.776614]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "813 [D loss: 0.608318, acc.: 59.38%] [G loss: 0.777404]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "814 [D loss: 0.635439, acc.: 68.75%] [G loss: 0.797408]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "815 [D loss: 0.620285, acc.: 68.75%] [G loss: 0.804526]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "816 [D loss: 0.679884, acc.: 50.00%] [G loss: 0.763995]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "817 [D loss: 0.624232, acc.: 68.75%] [G loss: 0.796633]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "818 [D loss: 0.631436, acc.: 62.50%] [G loss: 0.777661]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "819 [D loss: 0.672671, acc.: 65.62%] [G loss: 0.794739]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "820 [D loss: 0.713054, acc.: 43.75%] [G loss: 0.865963]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "821 [D loss: 0.699253, acc.: 53.12%] [G loss: 0.872935]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "822 [D loss: 0.625857, acc.: 65.62%] [G loss: 0.869507]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "823 [D loss: 0.703264, acc.: 50.00%] [G loss: 0.816788]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "824 [D loss: 0.544114, acc.: 78.12%] [G loss: 0.781874]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "825 [D loss: 0.683219, acc.: 46.88%] [G loss: 0.779818]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "826 [D loss: 0.651349, acc.: 59.38%] [G loss: 0.715731]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "827 [D loss: 0.744260, acc.: 40.62%] [G loss: 0.733257]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "828 [D loss: 0.669915, acc.: 53.12%] [G loss: 0.739154]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "829 [D loss: 0.629339, acc.: 65.62%] [G loss: 0.753450]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "830 [D loss: 0.693389, acc.: 53.12%] [G loss: 0.725615]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "831 [D loss: 0.634257, acc.: 59.38%] [G loss: 0.723756]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "832 [D loss: 0.675007, acc.: 62.50%] [G loss: 0.781085]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "833 [D loss: 0.674616, acc.: 53.12%] [G loss: 0.800211]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "834 [D loss: 0.565244, acc.: 78.12%] [G loss: 0.774456]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "835 [D loss: 0.738733, acc.: 43.75%] [G loss: 0.739598]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "836 [D loss: 0.657035, acc.: 50.00%] [G loss: 0.743688]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "837 [D loss: 0.693653, acc.: 62.50%] [G loss: 0.759061]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "838 [D loss: 0.585114, acc.: 78.12%] [G loss: 0.766246]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "839 [D loss: 0.675131, acc.: 59.38%] [G loss: 0.803900]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "840 [D loss: 0.673411, acc.: 62.50%] [G loss: 0.790105]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "841 [D loss: 0.655747, acc.: 59.38%] [G loss: 0.794762]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "842 [D loss: 0.672297, acc.: 46.88%] [G loss: 0.811647]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "843 [D loss: 0.657220, acc.: 68.75%] [G loss: 0.810455]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "844 [D loss: 0.633561, acc.: 62.50%] [G loss: 0.802257]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "845 [D loss: 0.685156, acc.: 53.12%] [G loss: 0.776664]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "846 [D loss: 0.669627, acc.: 50.00%] [G loss: 0.761561]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "847 [D loss: 0.622923, acc.: 81.25%] [G loss: 0.746351]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "848 [D loss: 0.608451, acc.: 71.88%] [G loss: 0.751534]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "849 [D loss: 0.631576, acc.: 65.62%] [G loss: 0.746409]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "850 [D loss: 0.628046, acc.: 65.62%] [G loss: 0.758623]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "851 [D loss: 0.693391, acc.: 50.00%] [G loss: 0.758709]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "852 [D loss: 0.657849, acc.: 62.50%] [G loss: 0.775345]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "853 [D loss: 0.561853, acc.: 84.38%] [G loss: 0.809447]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "854 [D loss: 0.635360, acc.: 59.38%] [G loss: 0.814133]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "855 [D loss: 0.625131, acc.: 59.38%] [G loss: 0.811846]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "856 [D loss: 0.647542, acc.: 62.50%] [G loss: 0.769803]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "857 [D loss: 0.648297, acc.: 46.88%] [G loss: 0.784206]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "858 [D loss: 0.626044, acc.: 75.00%] [G loss: 0.769915]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "859 [D loss: 0.612767, acc.: 68.75%] [G loss: 0.744721]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "860 [D loss: 0.629778, acc.: 68.75%] [G loss: 0.755021]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "861 [D loss: 0.693369, acc.: 59.38%] [G loss: 0.761133]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "862 [D loss: 0.640885, acc.: 71.88%] [G loss: 0.783139]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "863 [D loss: 0.640112, acc.: 71.88%] [G loss: 0.782799]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "864 [D loss: 0.653031, acc.: 56.25%] [G loss: 0.760602]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "865 [D loss: 0.623205, acc.: 68.75%] [G loss: 0.744819]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "866 [D loss: 0.655192, acc.: 50.00%] [G loss: 0.743153]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "867 [D loss: 0.646315, acc.: 65.62%] [G loss: 0.747472]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "868 [D loss: 0.657940, acc.: 62.50%] [G loss: 0.748503]\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "869 [D loss: 0.633140, acc.: 68.75%] [G loss: 0.759532]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "870 [D loss: 0.677112, acc.: 59.38%] [G loss: 0.794830]\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "871 [D loss: 0.639223, acc.: 59.38%] [G loss: 0.799302]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "872 [D loss: 0.666659, acc.: 65.62%] [G loss: 0.795778]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "873 [D loss: 0.642010, acc.: 65.62%] [G loss: 0.760229]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "874 [D loss: 0.629592, acc.: 59.38%] [G loss: 0.756894]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "875 [D loss: 0.659935, acc.: 59.38%] [G loss: 0.792312]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "876 [D loss: 0.622985, acc.: 65.62%] [G loss: 0.766309]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "877 [D loss: 0.627436, acc.: 75.00%] [G loss: 0.778692]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "878 [D loss: 0.659279, acc.: 53.12%] [G loss: 0.817932]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "879 [D loss: 0.595372, acc.: 81.25%] [G loss: 0.850410]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "880 [D loss: 0.689714, acc.: 43.75%] [G loss: 0.754063]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "881 [D loss: 0.633473, acc.: 62.50%] [G loss: 0.740720]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "882 [D loss: 0.680641, acc.: 62.50%] [G loss: 0.742102]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "883 [D loss: 0.650139, acc.: 59.38%] [G loss: 0.758161]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "884 [D loss: 0.639423, acc.: 68.75%] [G loss: 0.785389]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "885 [D loss: 0.637868, acc.: 65.62%] [G loss: 0.791067]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "886 [D loss: 0.631407, acc.: 71.88%] [G loss: 0.809806]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "887 [D loss: 0.684631, acc.: 62.50%] [G loss: 0.785966]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "888 [D loss: 0.637016, acc.: 62.50%] [G loss: 0.798541]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "889 [D loss: 0.625997, acc.: 65.62%] [G loss: 0.750891]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "890 [D loss: 0.640182, acc.: 56.25%] [G loss: 0.781999]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "891 [D loss: 0.675018, acc.: 46.88%] [G loss: 0.766404]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "892 [D loss: 0.611352, acc.: 65.62%] [G loss: 0.798543]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "893 [D loss: 0.636915, acc.: 62.50%] [G loss: 0.796810]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "894 [D loss: 0.642254, acc.: 65.62%] [G loss: 0.770083]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "895 [D loss: 0.669477, acc.: 59.38%] [G loss: 0.774761]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "896 [D loss: 0.662219, acc.: 56.25%] [G loss: 0.762444]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "897 [D loss: 0.635078, acc.: 65.62%] [G loss: 0.769578]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "898 [D loss: 0.656497, acc.: 59.38%] [G loss: 0.759871]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "899 [D loss: 0.685208, acc.: 53.12%] [G loss: 0.838477]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "900 [D loss: 0.591008, acc.: 78.12%] [G loss: 0.856048]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "901 [D loss: 0.612850, acc.: 87.50%] [G loss: 0.834148]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "902 [D loss: 0.662059, acc.: 68.75%] [G loss: 0.814746]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "903 [D loss: 0.668568, acc.: 50.00%] [G loss: 0.791058]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "904 [D loss: 0.626549, acc.: 65.62%] [G loss: 0.755953]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "905 [D loss: 0.653757, acc.: 59.38%] [G loss: 0.767525]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "906 [D loss: 0.649768, acc.: 43.75%] [G loss: 0.773644]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "907 [D loss: 0.654961, acc.: 59.38%] [G loss: 0.771180]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "908 [D loss: 0.602886, acc.: 75.00%] [G loss: 0.790604]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "909 [D loss: 0.619196, acc.: 59.38%] [G loss: 0.804062]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "910 [D loss: 0.609175, acc.: 78.12%] [G loss: 0.800305]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "911 [D loss: 0.594119, acc.: 65.62%] [G loss: 0.863882]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "912 [D loss: 0.593842, acc.: 75.00%] [G loss: 0.861892]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "913 [D loss: 0.620911, acc.: 71.88%] [G loss: 0.826470]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "914 [D loss: 0.604222, acc.: 71.88%] [G loss: 0.810380]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "915 [D loss: 0.597819, acc.: 71.88%] [G loss: 0.786037]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "916 [D loss: 0.645333, acc.: 56.25%] [G loss: 0.790700]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "917 [D loss: 0.639716, acc.: 65.62%] [G loss: 0.755010]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "918 [D loss: 0.648963, acc.: 59.38%] [G loss: 0.787629]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "919 [D loss: 0.587321, acc.: 81.25%] [G loss: 0.775639]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "920 [D loss: 0.629224, acc.: 75.00%] [G loss: 0.813003]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "921 [D loss: 0.648523, acc.: 65.62%] [G loss: 0.808322]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "922 [D loss: 0.622636, acc.: 71.88%] [G loss: 0.786605]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "923 [D loss: 0.643706, acc.: 65.62%] [G loss: 0.766272]\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "924 [D loss: 0.686990, acc.: 50.00%] [G loss: 0.777018]\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "925 [D loss: 0.638790, acc.: 62.50%] [G loss: 0.787266]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "926 [D loss: 0.679260, acc.: 56.25%] [G loss: 0.766920]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "927 [D loss: 0.648709, acc.: 56.25%] [G loss: 0.759135]\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "928 [D loss: 0.658053, acc.: 68.75%] [G loss: 0.758581]\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "929 [D loss: 0.664710, acc.: 62.50%] [G loss: 0.766522]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "930 [D loss: 0.628853, acc.: 65.62%] [G loss: 0.802440]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "931 [D loss: 0.645617, acc.: 62.50%] [G loss: 0.831853]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "932 [D loss: 0.602007, acc.: 81.25%] [G loss: 0.822059]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "933 [D loss: 0.662058, acc.: 56.25%] [G loss: 0.783285]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "934 [D loss: 0.648646, acc.: 68.75%] [G loss: 0.799977]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "935 [D loss: 0.631516, acc.: 62.50%] [G loss: 0.819102]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "936 [D loss: 0.673572, acc.: 56.25%] [G loss: 0.795946]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "937 [D loss: 0.680442, acc.: 56.25%] [G loss: 0.836539]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "938 [D loss: 0.579457, acc.: 81.25%] [G loss: 0.914270]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "939 [D loss: 0.651358, acc.: 62.50%] [G loss: 0.845134]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "940 [D loss: 0.608852, acc.: 75.00%] [G loss: 0.818447]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "941 [D loss: 0.681712, acc.: 59.38%] [G loss: 0.771262]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "942 [D loss: 0.640280, acc.: 62.50%] [G loss: 0.783065]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "943 [D loss: 0.640435, acc.: 62.50%] [G loss: 0.795865]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "944 [D loss: 0.682574, acc.: 53.12%] [G loss: 0.733367]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "945 [D loss: 0.655086, acc.: 59.38%] [G loss: 0.755947]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "946 [D loss: 0.670834, acc.: 65.62%] [G loss: 0.751007]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "947 [D loss: 0.612582, acc.: 65.62%] [G loss: 0.765521]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "948 [D loss: 0.660788, acc.: 65.62%] [G loss: 0.794011]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "949 [D loss: 0.643914, acc.: 56.25%] [G loss: 0.766875]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "950 [D loss: 0.617892, acc.: 78.12%] [G loss: 0.749504]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "951 [D loss: 0.651716, acc.: 62.50%] [G loss: 0.743744]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "952 [D loss: 0.579571, acc.: 71.88%] [G loss: 0.768224]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "953 [D loss: 0.681326, acc.: 62.50%] [G loss: 0.782129]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "954 [D loss: 0.595714, acc.: 71.88%] [G loss: 0.797762]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "955 [D loss: 0.644418, acc.: 68.75%] [G loss: 0.787382]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "956 [D loss: 0.646655, acc.: 62.50%] [G loss: 0.747071]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "957 [D loss: 0.649810, acc.: 56.25%] [G loss: 0.792754]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "958 [D loss: 0.651990, acc.: 68.75%] [G loss: 0.748247]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "959 [D loss: 0.695719, acc.: 59.38%] [G loss: 0.766914]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "960 [D loss: 0.630207, acc.: 65.62%] [G loss: 0.773252]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "961 [D loss: 0.699189, acc.: 43.75%] [G loss: 0.790909]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "962 [D loss: 0.680294, acc.: 56.25%] [G loss: 0.802856]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "963 [D loss: 0.648267, acc.: 62.50%] [G loss: 0.769205]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "964 [D loss: 0.627962, acc.: 65.62%] [G loss: 0.800945]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "965 [D loss: 0.631278, acc.: 65.62%] [G loss: 0.783757]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "966 [D loss: 0.718740, acc.: 53.12%] [G loss: 0.812456]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "967 [D loss: 0.667280, acc.: 56.25%] [G loss: 0.778963]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "968 [D loss: 0.643005, acc.: 65.62%] [G loss: 0.779883]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "969 [D loss: 0.622385, acc.: 56.25%] [G loss: 0.813207]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "970 [D loss: 0.584586, acc.: 71.88%] [G loss: 0.842660]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "971 [D loss: 0.687169, acc.: 50.00%] [G loss: 0.751275]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "972 [D loss: 0.614108, acc.: 65.62%] [G loss: 0.753794]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "973 [D loss: 0.631517, acc.: 75.00%] [G loss: 0.765769]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "974 [D loss: 0.610143, acc.: 59.38%] [G loss: 0.786397]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "975 [D loss: 0.645542, acc.: 53.12%] [G loss: 0.802602]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "976 [D loss: 0.627707, acc.: 68.75%] [G loss: 0.826560]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "977 [D loss: 0.619430, acc.: 78.12%] [G loss: 0.838264]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "978 [D loss: 0.677290, acc.: 46.88%] [G loss: 0.842720]\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "979 [D loss: 0.610603, acc.: 81.25%] [G loss: 0.843004]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "980 [D loss: 0.626401, acc.: 75.00%] [G loss: 0.814011]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "981 [D loss: 0.604326, acc.: 68.75%] [G loss: 0.840280]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "982 [D loss: 0.620108, acc.: 71.88%] [G loss: 0.814667]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "983 [D loss: 0.599762, acc.: 68.75%] [G loss: 0.811414]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "984 [D loss: 0.565923, acc.: 71.88%] [G loss: 0.831240]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "985 [D loss: 0.622886, acc.: 71.88%] [G loss: 0.787628]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "986 [D loss: 0.700707, acc.: 46.88%] [G loss: 0.796858]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "987 [D loss: 0.528287, acc.: 87.50%] [G loss: 0.826190]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "988 [D loss: 0.630939, acc.: 62.50%] [G loss: 0.840815]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "989 [D loss: 0.625380, acc.: 62.50%] [G loss: 0.848412]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "990 [D loss: 0.660780, acc.: 56.25%] [G loss: 0.790182]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "991 [D loss: 0.632945, acc.: 53.12%] [G loss: 0.798265]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "992 [D loss: 0.637333, acc.: 59.38%] [G loss: 0.773725]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "993 [D loss: 0.598574, acc.: 68.75%] [G loss: 0.793043]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "994 [D loss: 0.690053, acc.: 59.38%] [G loss: 0.793636]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "995 [D loss: 0.595831, acc.: 68.75%] [G loss: 0.805856]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "996 [D loss: 0.656436, acc.: 59.38%] [G loss: 0.822112]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "997 [D loss: 0.652686, acc.: 56.25%] [G loss: 0.810138]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "998 [D loss: 0.634309, acc.: 59.38%] [G loss: 0.837574]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "999 [D loss: 0.675232, acc.: 62.50%] [G loss: 0.764709]\n"
          ]
        }
      ]
    }
  ]
}